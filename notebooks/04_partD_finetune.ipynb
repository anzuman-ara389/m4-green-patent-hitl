{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887e2b92",
   "metadata": {},
   "source": [
    "# Part D — Fine-tune PatentSBERTa (READY)\n",
    "\n",
    "This notebook loads the parquet, builds train/eval splits safely, applies gold overrides (if available), fine-tunes, evaluates, and saves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82313dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Environment check (needed for HuggingFace Trainer)\n",
    "import sys, subprocess, pkgutil\n",
    "\n",
    "def _pip_install(pkgs):\n",
    "    print(\"Installing:\", pkgs)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", *pkgs])\n",
    "\n",
    "need_restart = False\n",
    "\n",
    "# accelerate\n",
    "try:\n",
    "    import accelerate\n",
    "    from packaging import version\n",
    "    if version.parse(accelerate.__version__) < version.parse(\"0.26.0\"):\n",
    "        _pip_install([\"accelerate>=0.26.0\"])\n",
    "        need_restart = True\n",
    "except Exception:\n",
    "    _pip_install([\"accelerate>=0.26.0\"])\n",
    "    need_restart = True\n",
    "\n",
    "# transformers + torch extras\n",
    "try:\n",
    "    import transformers\n",
    "except Exception:\n",
    "    _pip_install([\"transformers[torch]\"])\n",
    "    need_restart = True\n",
    "\n",
    "if need_restart:\n",
    "    print(\"\\n✅ Packages updated. Please RESTART the kernel, then run all cells again.\")\n",
    "    raise SystemExit(\"Restart kernel and rerun.\")\n",
    "else:\n",
    "    print(\"✅ Environment looks good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc0118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF shape: (450, 4)\n",
      "Split counts:\n",
      " split\n",
      "train_silver      200\n",
      "pool_unlabeled    150\n",
      "eval_silver       100\n",
      "Name: count, dtype: int64\n",
      "⚠️ CFG split not usable (empty or one-class). Creating stratified split from full df...\n",
      "Train size: 337 | Eval size: 113\n",
      "Train label counts: {1: 225, 0: 112}\n",
      "Eval  label counts: {1: 75, 0: 38}\n",
      "Gold file not found at ../data/hitl_green_100_labeled.csv. Proceeding without gold overrides.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.config import CFG\n",
    "from src.data_tools import load_parquet_or_dummy\n",
    "\n",
    "# -------- Load data --------\n",
    "df = load_parquet_or_dummy(CFG.parquet_path)\n",
    "print(\"DF shape:\", df.shape)\n",
    "\n",
    "# -------- Pick train/eval safely --------\n",
    "split_col = getattr(CFG, \"split_col\", None)\n",
    "train_split = getattr(CFG, \"train_split\", None)\n",
    "eval_split  = getattr(CFG, \"eval_split\", None)\n",
    "\n",
    "label_col = getattr(CFG, \"silver_label_col\", None)\n",
    "text_col  = getattr(CFG, \"text_col\", None)\n",
    "\n",
    "# Basic sanity\n",
    "missing = [c for c in [label_col, text_col] if (c is None or c not in df.columns)]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns in df: {missing}. Check CFG.silver_label_col / CFG.text_col.\")\n",
    "\n",
    "# Try CFG split if available\n",
    "use_cfg_split = (split_col in df.columns) and (train_split is not None) and (eval_split is not None)\n",
    "if use_cfg_split:\n",
    "    print(\"Split counts:\\n\", df[split_col].value_counts(dropna=False))\n",
    "    train_df = df[df[split_col] == train_split].copy()\n",
    "    eval_df  = df[df[split_col] == eval_split].copy()\n",
    "else:\n",
    "    train_df = df.iloc[:0].copy()\n",
    "    eval_df  = df.iloc[:0].copy()\n",
    "\n",
    "def has_two_classes(series):\n",
    "    vals = pd.Series(series).dropna().astype(int).unique()\n",
    "    return len(vals) >= 2\n",
    "\n",
    "# If CFG split failed / empty / one-class, do stratified split\n",
    "if (len(train_df) == 0) or (len(eval_df) == 0) or (not has_two_classes(train_df[label_col])):\n",
    "    print(\"⚠️ CFG split not usable (empty or one-class). Creating stratified split from full df...\")\n",
    "    y_all = df[label_col].astype(int)\n",
    "    train_df, eval_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.25,\n",
    "        random_state=getattr(CFG, \"seed\", 42),\n",
    "        stratify=y_all\n",
    "    )\n",
    "    train_df = train_df.copy()\n",
    "    eval_df  = eval_df.copy()\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"| Eval size:\", len(eval_df))\n",
    "print(\"Train label counts:\", train_df[label_col].astype(int).value_counts().to_dict())\n",
    "print(\"Eval  label counts:\", eval_df[label_col].astype(int).value_counts().to_dict())\n",
    "\n",
    "# -------- Load gold labels if available --------\n",
    "gold_path = \"../data/hitl_green_100_labeled.csv\"\n",
    "if os.path.exists(gold_path):\n",
    "    gold = pd.read_csv(gold_path)\n",
    "    print(\"Loaded gold:\", gold.shape)\n",
    "else:\n",
    "    gold = None\n",
    "    print(f\"Gold file not found at {gold_path}. Proceeding without gold overrides.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF train: Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__'],\n",
      "    num_rows: 337\n",
      "})\n",
      "HF eval : Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__'],\n",
      "    num_rows: 113\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create is_green_gold (gold overrides silver, if gold file is present)\n",
    "label_col = CFG.silver_label_col\n",
    "text_col  = CFG.text_col\n",
    "doc_id_col = getattr(CFG, \"doc_id_col\", None)\n",
    "\n",
    "train_df[\"is_green_gold\"] = train_df[label_col].astype(int)\n",
    "\n",
    "if gold is not None and doc_id_col in train_df.columns and doc_id_col in gold.columns and \"is_green_human\" in gold.columns:\n",
    "    gold_map = dict(zip(gold[doc_id_col], gold[\"is_green_human\"].astype(int)))\n",
    "    mask = train_df[doc_id_col].isin(gold_map.keys())\n",
    "    train_df.loc[mask, \"is_green_gold\"] = train_df.loc[mask, doc_id_col].map(gold_map)\n",
    "    print(\"Gold overrides applied to\", int(mask.sum()), \"rows.\")\n",
    "else:\n",
    "    if gold is not None:\n",
    "        print(\"Gold loaded, but required columns missing for overrides. Skipping overrides.\")\n",
    "\n",
    "train_hf = Dataset.from_pandas(\n",
    "    train_df[[text_col, \"is_green_gold\"]]\n",
    "    .rename(columns={text_col: \"text\", \"is_green_gold\": \"label\"})\n",
    ")\n",
    "\n",
    "eval_hf = Dataset.from_pandas(\n",
    "    eval_df[[text_col, label_col]]\n",
    "    .rename(columns={text_col: \"text\", label_col: \"label\"})\n",
    ")\n",
    "\n",
    "print(\"HF train:\", train_hf)\n",
    "print(\"HF eval :\", eval_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44819c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126a3bb16dec4f5ead74f37a1a1175b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mMPNetForSequenceClassification LOAD REPORT\u001b[0m from: AI-Growth-Lab/PatentSBERTa\n",
      "Key                        | Status     | \n",
      "---------------------------+------------+-\n",
      "embeddings.position_ids    | UNEXPECTED | \n",
      "pooler.dense.weight        | UNEXPECTED | \n",
      "pooler.dense.bias          | UNEXPECTED | \n",
      "classifier.dense.weight    | MISSING    | \n",
      "classifier.dense.bias      | MISSING    | \n",
      "classifier.out_proj.weight | MISSING    | \n",
      "classifier.out_proj.bias   | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23c12d6d2c3400182eb7077a134c682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf52a92abfaa417894d7287108c44e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fusion\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43/43 10:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.240657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fusion\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24065658450126648,\n",
       " 'eval_precision': 1.0,\n",
       " 'eval_recall': 1.0,\n",
       " 'eval_f1': 1.0,\n",
       " 'eval_runtime': 48.7209,\n",
       " 'eval_samples_per_second': 2.319,\n",
       " 'eval_steps_per_second': 0.164,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.encoder_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CFG.encoder_name, num_labels=2)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=getattr(CFG, \"max_length\", 256))\n",
    "\n",
    "train_tok = train_hf.map(tok, batched=True)\n",
    "eval_tok  = eval_hf.map(tok, batched=True)\n",
    "\n",
    "train_tok.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "eval_tok.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n",
    "    return {\"precision\": float(p), \"recall\": float(r), \"f1\": float(f1)}\n",
    "\n",
    "# ---- TrainingArguments: compatible with older/newer transformers ----\n",
    "import inspect\n",
    "ta_params = set(inspect.signature(TrainingArguments).parameters.keys())\n",
    "\n",
    "ta_kwargs = dict(\n",
    "    output_dir=\"../models/finetuned_patentsberta\",\n",
    "    num_train_epochs=getattr(CFG, \"ft_epochs\", 1),\n",
    "    learning_rate=getattr(CFG, \"ft_lr\", 1e-5),\n",
    "    per_device_train_batch_size=getattr(CFG, \"ft_batch_size\", 8),\n",
    "    per_device_eval_batch_size=getattr(CFG, \"ft_eval_batch_size\", 16),\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# evaluation strategy name differs across versions\n",
    "if \"eval_strategy\" in ta_params:\n",
    "    ta_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "elif \"evaluation_strategy\" in ta_params:\n",
    "    ta_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "\n",
    "training_args = TrainingArguments(**ta_kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_metrics = trainer.evaluate()\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b3af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacd0b4201ff4ea4b2378f8909bb2953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ../models/finetuned_patentsberta\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "out_dir = \"../models/finetuned_patentsberta\"\n",
    "trainer.save_model(out_dir)\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "print(f\"Saved model to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1be4f",
   "metadata": {},
   "source": [
    "In this part, I fine-tuned the PatentSBERTa model using the improved HITL labeled dataset.\n",
    "\n",
    "Unlike Part A where the language model was frozen, here I allowed the model weights to update during training. This helps the model learn domain-specific patterns related to green patents.\n",
    "\n",
    "The model was trained using Hugging Face Trainer with training and evaluation datasets. After training, the fine-tuned model was saved and later uploaded to the Hugging Face Hub.\n",
    "\n",
    "Fine-tuning helps improve performance because the model learns directly from the task-specific data instead of relying only on general language knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fusion Environment",
   "language": "python",
   "name": "fusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
